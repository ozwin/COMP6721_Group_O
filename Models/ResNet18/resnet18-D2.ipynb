{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f5d6e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from google.colab import files\n",
    "from google.colab import drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32995de",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')\n",
    "from zipfile import ZipFile\n",
    "file_name = \"/content/drive/MyDrive/35-classes.zip\"\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "  zip.extractall()\n",
    "  print('Done')\n",
    "\n",
    "picture = plt.imread('/content/35-classes/train/agkistrodon-contortrix/0c1f3ddd69.jpg')\n",
    "imageShow = plt.imshow(picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db5b52da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ozwin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ozwin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 162, 162]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 162, 162]             128\n",
      "              ReLU-3         [-1, 64, 162, 162]               0\n",
      "         MaxPool2d-4           [-1, 64, 81, 81]               0\n",
      "            Conv2d-5           [-1, 64, 81, 81]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 81, 81]             128\n",
      "              ReLU-7           [-1, 64, 81, 81]               0\n",
      "            Conv2d-8           [-1, 64, 81, 81]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 81, 81]             128\n",
      "             ReLU-10           [-1, 64, 81, 81]               0\n",
      "       BasicBlock-11           [-1, 64, 81, 81]               0\n",
      "           Conv2d-12           [-1, 64, 81, 81]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 81, 81]             128\n",
      "             ReLU-14           [-1, 64, 81, 81]               0\n",
      "           Conv2d-15           [-1, 64, 81, 81]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 81, 81]             128\n",
      "             ReLU-17           [-1, 64, 81, 81]               0\n",
      "       BasicBlock-18           [-1, 64, 81, 81]               0\n",
      "           Conv2d-19          [-1, 128, 41, 41]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 41, 41]             256\n",
      "             ReLU-21          [-1, 128, 41, 41]               0\n",
      "           Conv2d-22          [-1, 128, 41, 41]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 41, 41]             256\n",
      "           Conv2d-24          [-1, 128, 41, 41]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 41, 41]             256\n",
      "             ReLU-26          [-1, 128, 41, 41]               0\n",
      "       BasicBlock-27          [-1, 128, 41, 41]               0\n",
      "           Conv2d-28          [-1, 128, 41, 41]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 41, 41]             256\n",
      "             ReLU-30          [-1, 128, 41, 41]               0\n",
      "           Conv2d-31          [-1, 128, 41, 41]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 41, 41]             256\n",
      "             ReLU-33          [-1, 128, 41, 41]               0\n",
      "       BasicBlock-34          [-1, 128, 41, 41]               0\n",
      "           Conv2d-35          [-1, 256, 21, 21]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 21, 21]             512\n",
      "             ReLU-37          [-1, 256, 21, 21]               0\n",
      "           Conv2d-38          [-1, 256, 21, 21]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 21, 21]             512\n",
      "           Conv2d-40          [-1, 256, 21, 21]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 21, 21]             512\n",
      "             ReLU-42          [-1, 256, 21, 21]               0\n",
      "       BasicBlock-43          [-1, 256, 21, 21]               0\n",
      "           Conv2d-44          [-1, 256, 21, 21]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 21, 21]             512\n",
      "             ReLU-46          [-1, 256, 21, 21]               0\n",
      "           Conv2d-47          [-1, 256, 21, 21]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 21, 21]             512\n",
      "             ReLU-49          [-1, 256, 21, 21]               0\n",
      "       BasicBlock-50          [-1, 256, 21, 21]               0\n",
      "           Conv2d-51          [-1, 512, 11, 11]       1,179,648\n",
      "      BatchNorm2d-52          [-1, 512, 11, 11]           1,024\n",
      "             ReLU-53          [-1, 512, 11, 11]               0\n",
      "           Conv2d-54          [-1, 512, 11, 11]       2,359,296\n",
      "      BatchNorm2d-55          [-1, 512, 11, 11]           1,024\n",
      "           Conv2d-56          [-1, 512, 11, 11]         131,072\n",
      "      BatchNorm2d-57          [-1, 512, 11, 11]           1,024\n",
      "             ReLU-58          [-1, 512, 11, 11]               0\n",
      "       BasicBlock-59          [-1, 512, 11, 11]               0\n",
      "           Conv2d-60          [-1, 512, 11, 11]       2,359,296\n",
      "      BatchNorm2d-61          [-1, 512, 11, 11]           1,024\n",
      "             ReLU-62          [-1, 512, 11, 11]               0\n",
      "           Conv2d-63          [-1, 512, 11, 11]       2,359,296\n",
      "      BatchNorm2d-64          [-1, 512, 11, 11]           1,024\n",
      "             ReLU-65          [-1, 512, 11, 11]               0\n",
      "       BasicBlock-66          [-1, 512, 11, 11]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                    [-1, 5]           2,565\n",
      "================================================================\n",
      "Total params: 11,179,077\n",
      "Trainable params: 11,179,077\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.20\n",
      "Forward/backward pass size (MB): 134.11\n",
      "Params size (MB): 42.64\n",
      "Estimated Total Size (MB): 177.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "resnet18Model=models.resnet18(pretrained=False)\n",
    "num_filters = resnet18Model.fc.in_features\n",
    "num_classes = 35 \n",
    "resnet18Model.fc = nn.Linear(num_filters,num_classes)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(resnet18Model)\n",
    "# summary(resnet18Model,input_size=(3,324,324))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet18Model.to(device)\n",
    "print(\"Device: {}\".format(device))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(resnet18Model.parameters(), lr=0.0002,momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=0.0001)\n",
    "n_epochs = 100\n",
    "# resnet18Model.load_state_dict(torch.load('/content/82-epoch-output.pt'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744a7e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import this file to train models and pass the  name of the datset\n",
    "# extract zip files\n",
    "# load the images and labels based on the dataset requested\n",
    "# do the pre-processing , batch normalization , flip etc\n",
    "\n",
    "# Transofrmations for preprocessedSnakeImages dataset\n",
    "# Reference from https://www.youtube.com/watch?v=z3kB3ISIPAg&list=PL3Dh_99BJkCEhE7Ri8W6aijiEqm3ZoGRq&index=4\n",
    "training_path = '/content/35-classes/train/'\n",
    "test_path = '/content/35-classes/test'\n",
    "val_path = '/content/35-classes/val'\n",
    "def transformDS1( batchSize, inputSize):\n",
    "\n",
    "    training_transforms = transforms.Compose([transforms.Resize((inputSize,inputSize)),transforms.ToTensor()])\n",
    "    train_dataset = torchvision.datasets.ImageFolder(root=training_path,transform = training_transforms)\n",
    "    train_Loader = torch.utils.data.DataLoader(dataset = train_dataset,batch_size=batchSize,shuffle=False)\n",
    "    mean, std = get_mean_std(train_Loader)\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize((inputSize,inputSize)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(torch.Tensor(mean),torch.Tensor(std))\n",
    "    ])\n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.Resize((inputSize,inputSize)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(torch.Tensor(mean),torch.Tensor(std))\n",
    "    ])\n",
    "\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize((inputSize,inputSize)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(torch.Tensor(mean),torch.Tensor(std))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.ImageFolder(root=training_path,transform=train_transforms)\n",
    "    test_dataset = torchvision.datasets.ImageFolder(root=test_path,transform=test_transforms)\n",
    "    val_dataset = torchvision.datasets.ImageFolder(root=val_path,transform=val_transforms)\n",
    "\n",
    "    data_loader_train = torch.utils.data.DataLoader(train_dataset, batch_size=batchSize,\n",
    "    shuffle=True,drop_last=False,num_workers=0)\n",
    "    data_loader_test = torch.utils.data.DataLoader(test_dataset, batch_size=batchSize,\n",
    "    shuffle=True,drop_last=False,num_workers=0)\n",
    "    data_loader_val = torch.utils.data.DataLoader(val_dataset, batch_size=batchSize,\n",
    "    shuffle=True,drop_last=False,num_workers=0)\n",
    "\n",
    "    return data_loader_train,data_loader_test,data_loader_val\n",
    "\n",
    "def get_mean_std(loader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total_images_count = 0\n",
    "    for images,_ in loader:\n",
    "        images_count_in_batch = images.size(0)\n",
    "        images =images.view(images_count_in_batch,images.size(1),-1)\n",
    "        mean+=images.mean(2).sum(0)\n",
    "        std+=images.std(2).sum(0)\n",
    "        total_images_count+=images_count_in_batch\n",
    "    mean /= total_images_count\n",
    "    std /= total_images_count\n",
    "    return mean,std\n",
    "\n",
    "def show_transformed_images(data_loader_train):\n",
    "    batch=next(iter(data_loader_train))\n",
    "    images,labels = batch\n",
    "    grid = torchvision.utils.make_grid(images,nrow=3)\n",
    "    plt.figure(figsize=(11,11))\n",
    "    plt.imshow(np.transpose(grid,(1,2,0)))\n",
    "    plt.show()\n",
    "    print(\"labels:\",labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0618cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train,data_loader_test,data_loader_val = transformDS1(32,224)\n",
    "show_transformed_images(data_loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e3e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracies=[]\n",
    "valAccuracies = []\n",
    "total_steps = len(data_loader_train)\n",
    "t1 = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Epoch number %d\" %(epoch+1))\n",
    "    resnet18Model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0.0\n",
    "    total = 0\n",
    "    for i, data in enumerate(data_loader_train):\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet18Model(images)\n",
    "        _,predicted = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            validation_accuracy = 0\n",
    "            resnet18Model.eval()\n",
    "            with torch.no_grad(): \n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                for data in data_loader_val:\n",
    "                    images, val_labels = data[0].to(device), data[1].to(device)\n",
    "                    outputs = resnet18Model(images)\n",
    "                    # Validation set accuracy\n",
    "                    _,predicted = torch.max(outputs.data, 1)\n",
    "                    val_correct += (predicted == val_labels).sum().item()\n",
    "                    val_total  += val_labels.size(0)\n",
    "\n",
    "            validation_accuracy = (val_correct / val_total)*100\n",
    "            training_accuracy = (running_correct / total) * 100\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Training Accuracy: {:.2f}%, Validation Accuracy: {:.2f}%'\n",
    "                .format(epoch + 1, n_epochs, i + 1, total_steps, loss.item(), training_accuracy , validation_accuracy))\n",
    "    scheduler.step()\n",
    "    epoch_loss = running_loss/len(data_loader_train)\n",
    "    epoch_accuracy = 100.00 * (running_correct/total)   \n",
    "    Accuracies.append(epoch_accuracy)\n",
    "    valAccuracies.append(validation_accuracy)      \n",
    "    print(\"Training Data: Epoch Loss: %.3f, Epoch Accuracy: %.3f\"%(epoch_loss,epoch_accuracy))\n",
    "      \n",
    "print(\"######## Training Finished in {} seconds ###########\".format(time.time()-t1))\n",
    "fileName=f'/content/35-class-d2-100-epochs.pt'\n",
    "torch.save(resnet18Model.state_dict(), fileName)\n",
    "resnet18Model.eval()\n",
    "predicted_correct =0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in data_loader_test:\n",
    "        images,labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        total+=labels.size(0)\n",
    "        outputs = resnet18Model(images)\n",
    "        _,predicted = torch.max(outputs,1)\n",
    "        predicted_correct += (predicted == labels).sum().item()\n",
    "epoch_accuracy = 100.0* predicted_correct/total\n",
    "print(\"Testing Data: Epoch Accuracy: %.3f\"%(epoch_accuracy))\n",
    "dict = {'Training': Accuracies, 'Validation': valAccuracies} \n",
    "df = pd.DataFrame(dict)\n",
    "df.to_csv(\"/content/Accuracies-D2-resnet18.csv\")\n",
    "files.download(fileName)\n",
    "files.download('/content/Accuracies-D2-resnet18.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c562765b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9057d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Accuracies Vs Epochs\n",
    "plt.plot(range(n_epochs),Accuracies)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Accuracies\")\n",
    "plt.title(\"Training Accuracy Vs Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dfabaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Validation Accuracies Vs Train Accuracies on Epochs\n",
    "plt.plot(range(n_epochs),Accuracies, label=\"Training\")\n",
    "plt.plot(range(n_epochs),valAccuracies, label=\"Validation\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracies\")\n",
    "plt.title(\"Training vs Validation Accuracies\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d48f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over test data\n",
    "for data in data_loader_test:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        output = resnet18Model(inputs) # Feed Network\n",
    "\n",
    "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "        y_pred.extend(output) # Save Prediction\n",
    "        \n",
    "        labels = labels.data.cpu().numpy()\n",
    "        y_true.extend(labels) # Save Truth\n",
    "\n",
    "# constant for classes\n",
    "classes =(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35)\n",
    "\n",
    "# Build confusion matrix\n",
    "confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(y_true, y_pred),\n",
    "                              display_labels=classes)\n",
    "disp.plot()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "27c12e6cacbf7604865d82dba11605bee4bbecd8b82f9ede11b3d85a68a54965"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
