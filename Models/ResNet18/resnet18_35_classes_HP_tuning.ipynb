{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f5d6e24",
      "metadata": {
        "id": "1f5d6e24"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision.models as models\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "from google.colab import files\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e32995de",
      "metadata": {
        "id": "e32995de"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "from zipfile import ZipFile\n",
        "file_name = \"/content/drive/MyDrive/35-classes.zip\"\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')\n",
        "\n",
        "picture = plt.imread('/content/35-classes/train/agkistrodon-contortrix/0c1f3ddd69.jpg')\n",
        "imageShow = plt.imshow(picture)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db5b52da",
      "metadata": {
        "id": "db5b52da",
        "outputId": "1b4571c9-1856-41a6-8ea6-c7a84789b113"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ozwin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\ozwin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 162, 162]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 162, 162]             128\n",
            "              ReLU-3         [-1, 64, 162, 162]               0\n",
            "         MaxPool2d-4           [-1, 64, 81, 81]               0\n",
            "            Conv2d-5           [-1, 64, 81, 81]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 81, 81]             128\n",
            "              ReLU-7           [-1, 64, 81, 81]               0\n",
            "            Conv2d-8           [-1, 64, 81, 81]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 81, 81]             128\n",
            "             ReLU-10           [-1, 64, 81, 81]               0\n",
            "       BasicBlock-11           [-1, 64, 81, 81]               0\n",
            "           Conv2d-12           [-1, 64, 81, 81]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 81, 81]             128\n",
            "             ReLU-14           [-1, 64, 81, 81]               0\n",
            "           Conv2d-15           [-1, 64, 81, 81]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 81, 81]             128\n",
            "             ReLU-17           [-1, 64, 81, 81]               0\n",
            "       BasicBlock-18           [-1, 64, 81, 81]               0\n",
            "           Conv2d-19          [-1, 128, 41, 41]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 41, 41]             256\n",
            "             ReLU-21          [-1, 128, 41, 41]               0\n",
            "           Conv2d-22          [-1, 128, 41, 41]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 41, 41]             256\n",
            "           Conv2d-24          [-1, 128, 41, 41]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 41, 41]             256\n",
            "             ReLU-26          [-1, 128, 41, 41]               0\n",
            "       BasicBlock-27          [-1, 128, 41, 41]               0\n",
            "           Conv2d-28          [-1, 128, 41, 41]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 41, 41]             256\n",
            "             ReLU-30          [-1, 128, 41, 41]               0\n",
            "           Conv2d-31          [-1, 128, 41, 41]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 41, 41]             256\n",
            "             ReLU-33          [-1, 128, 41, 41]               0\n",
            "       BasicBlock-34          [-1, 128, 41, 41]               0\n",
            "           Conv2d-35          [-1, 256, 21, 21]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 21, 21]             512\n",
            "             ReLU-37          [-1, 256, 21, 21]               0\n",
            "           Conv2d-38          [-1, 256, 21, 21]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 21, 21]             512\n",
            "           Conv2d-40          [-1, 256, 21, 21]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 21, 21]             512\n",
            "             ReLU-42          [-1, 256, 21, 21]               0\n",
            "       BasicBlock-43          [-1, 256, 21, 21]               0\n",
            "           Conv2d-44          [-1, 256, 21, 21]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 21, 21]             512\n",
            "             ReLU-46          [-1, 256, 21, 21]               0\n",
            "           Conv2d-47          [-1, 256, 21, 21]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 21, 21]             512\n",
            "             ReLU-49          [-1, 256, 21, 21]               0\n",
            "       BasicBlock-50          [-1, 256, 21, 21]               0\n",
            "           Conv2d-51          [-1, 512, 11, 11]       1,179,648\n",
            "      BatchNorm2d-52          [-1, 512, 11, 11]           1,024\n",
            "             ReLU-53          [-1, 512, 11, 11]               0\n",
            "           Conv2d-54          [-1, 512, 11, 11]       2,359,296\n",
            "      BatchNorm2d-55          [-1, 512, 11, 11]           1,024\n",
            "           Conv2d-56          [-1, 512, 11, 11]         131,072\n",
            "      BatchNorm2d-57          [-1, 512, 11, 11]           1,024\n",
            "             ReLU-58          [-1, 512, 11, 11]               0\n",
            "       BasicBlock-59          [-1, 512, 11, 11]               0\n",
            "           Conv2d-60          [-1, 512, 11, 11]       2,359,296\n",
            "      BatchNorm2d-61          [-1, 512, 11, 11]           1,024\n",
            "             ReLU-62          [-1, 512, 11, 11]               0\n",
            "           Conv2d-63          [-1, 512, 11, 11]       2,359,296\n",
            "      BatchNorm2d-64          [-1, 512, 11, 11]           1,024\n",
            "             ReLU-65          [-1, 512, 11, 11]               0\n",
            "       BasicBlock-66          [-1, 512, 11, 11]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                    [-1, 5]           2,565\n",
            "================================================================\n",
            "Total params: 11,179,077\n",
            "Trainable params: 11,179,077\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.20\n",
            "Forward/backward pass size (MB): 134.11\n",
            "Params size (MB): 42.64\n",
            "Estimated Total Size (MB): 177.96\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "resnet18Model=models.resnet18(pretrained=False)\n",
        "num_filters = resnet18Model.fc.in_features\n",
        "num_classes = 35 \n",
        "resnet18Model.fc = nn.Linear(num_filters,num_classes)\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "count_parameters(resnet18Model)\n",
        "# summary(resnet18Model,input_size=(3,324,324))\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "resnet18Model.to(device)\n",
        "print(\"Device: {}\".format(device))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(resnet18Model.parameters(), lr=0.0002,momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=0.0001)\n",
        "n_epochs = 100\n",
        "epoch_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "744a7e8c",
      "metadata": {
        "id": "744a7e8c"
      },
      "outputs": [],
      "source": [
        "\n",
        "#import this file to train models and pass the  name of the datset\n",
        "# extract zip files\n",
        "# load the images and labels based on the dataset requested\n",
        "# do the pre-processing , batch normalization , flip etc\n",
        "\n",
        "# Transofrmations for preprocessedSnakeImages dataset\n",
        "# Reference from https://www.youtube.com/watch?v=z3kB3ISIPAg&list=PL3Dh_99BJkCEhE7Ri8W6aijiEqm3ZoGRq&index=4\n",
        "training_path = '/content/35-classes/train/'\n",
        "test_path = '/content/35-classes/test'\n",
        "val_path = '/content/35-classes/val'\n",
        "def transformDS1( batchSize, inputSize):\n",
        "\n",
        "    training_transforms = transforms.Compose([transforms.Resize((inputSize,inputSize)),transforms.ToTensor()])\n",
        "    train_dataset = torchvision.datasets.ImageFolder(root=training_path,transform = training_transforms)\n",
        "    train_Loader = torch.utils.data.DataLoader(dataset = train_dataset,batch_size=batchSize,shuffle=False)\n",
        "    mean, std = get_mean_std(train_Loader)\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.Resize((inputSize,inputSize)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(torch.Tensor(mean),torch.Tensor(std))\n",
        "    ])\n",
        "\n",
        "    test_transforms = transforms.Compose([\n",
        "        transforms.Resize((inputSize,inputSize)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(torch.Tensor(mean),torch.Tensor(std))\n",
        "    ])\n",
        "\n",
        "    val_transforms = transforms.Compose([\n",
        "        transforms.Resize((inputSize,inputSize)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(torch.Tensor(mean),torch.Tensor(std))\n",
        "    ])\n",
        "    \n",
        "    train_dataset = torchvision.datasets.ImageFolder(root=training_path,transform=train_transforms)\n",
        "    test_dataset = torchvision.datasets.ImageFolder(root=test_path,transform=test_transforms)\n",
        "    val_dataset = torchvision.datasets.ImageFolder(root=val_path,transform=val_transforms)\n",
        "\n",
        "    data_loader_train = torch.utils.data.DataLoader(train_dataset, batch_size=batchSize,\n",
        "    shuffle=True,drop_last=False,num_workers=0)\n",
        "    data_loader_test = torch.utils.data.DataLoader(test_dataset, batch_size=batchSize,\n",
        "    shuffle=True,drop_last=False,num_workers=0)\n",
        "    data_loader_val = torch.utils.data.DataLoader(val_dataset, batch_size=batchSize,\n",
        "    shuffle=True,drop_last=False,num_workers=0)\n",
        "\n",
        "    return data_loader_train,data_loader_test,data_loader_val\n",
        "\n",
        "def get_mean_std(loader):\n",
        "    mean = 0.\n",
        "    std = 0.\n",
        "    total_images_count = 0\n",
        "    for images,_ in loader:\n",
        "        images_count_in_batch = images.size(0)\n",
        "        images =images.view(images_count_in_batch,images.size(1),-1)\n",
        "        mean+=images.mean(2).sum(0)\n",
        "        std+=images.std(2).sum(0)\n",
        "        total_images_count+=images_count_in_batch\n",
        "    mean /= total_images_count\n",
        "    std /= total_images_count\n",
        "    return mean,std\n",
        "\n",
        "def show_transformed_images(data_loader_train):\n",
        "    batch=next(iter(data_loader_train))\n",
        "    images,labels = batch\n",
        "    grid = torchvision.utils.make_grid(images,nrow=3)\n",
        "    plt.figure(figsize=(11,11))\n",
        "    plt.imshow(np.transpose(grid,(1,2,0)))\n",
        "    plt.show()\n",
        "    print(\"labels:\",labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bcff6f8",
      "metadata": {
        "id": "5bcff6f8"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model,test_loader):\n",
        "    model.eval()\n",
        "    predicted_correct =0\n",
        "    total = 0\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images,labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            total+=labels.size(0)\n",
        "            outputs = model(images)\n",
        "            _,predicted = torch.max(outputs,1)\n",
        "            predicted_correct += (predicted == labels).sum().item()\n",
        "    epoch_accuracy = 100.0* predicted_correct/total\n",
        "    print(\"Testing Data: Epoch Accuracy: %.3f\"%(epoch_accuracy))\n",
        "    return epoch_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0618cdbf",
      "metadata": {
        "id": "0618cdbf"
      },
      "outputs": [],
      "source": [
        "train_loader,test_loader,val_loader = transformDS1(32,224)\n",
        "show_transformed_images(val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10e3e6f1",
      "metadata": {
        "id": "10e3e6f1"
      },
      "outputs": [],
      "source": [
        "\n",
        "Accuracies = [] \n",
        "valAccuracies = []\n",
        "valLoss = []\n",
        "trainLoss = []\n",
        "total_steps = len(train_loader)\n",
        "t1 = time.time()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "validation_accuracy = 0\n",
        "print(\"-----Device:\"+device+\"-----\")\n",
        "for epoch in range(n_epochs):\n",
        "    print(\"Epoch number %d\" %(epoch+1))\n",
        "    resnet18Model.train()\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0.0\n",
        "    val_loss = 0.0\n",
        "    total = 0\n",
        "    for i, data in enumerate(train_loader):\n",
        "      images,labels = data\n",
        "      device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      total+=labels.size(0)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = resnet18Model(images)\n",
        "      _,predicted = torch.max(outputs.data,1)\n",
        "      loss = criterion(outputs,labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      running_loss+=loss.item()\n",
        "      running_correct += (labels==predicted).sum().item()\n",
        "     \n",
        "      \n",
        "      training_accuracy = (running_correct / total) * 100\n",
        "    validation_accuracy = 0\n",
        "    resnet18Model.eval()\n",
        "    with torch.no_grad(): \n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        val_loss = 0\n",
        "        for data in val_loader:\n",
        "            images, val_labels = data[0].to(device), data[1].to(device)\n",
        "            images = images.to(device)\n",
        "            val_labels = val_labels.to(device)\n",
        "            outputs = resnet18Model(images)\n",
        "            lossVal = criterion(outputs,val_labels)\n",
        "            val_loss += lossVal.item()\n",
        "            # Validation set accuracy\n",
        "            \n",
        "            _,predicted = torch.max(outputs.data, 1)\n",
        "            val_correct += (predicted == val_labels).sum().item()\n",
        "            val_total  += val_labels.size(0)\n",
        "          \n",
        "        validation_accuracy = (val_correct / val_total) *100\n",
        "\n",
        "    scheduler.step()  \n",
        "    epoch_loss = running_loss/len(train_loader)\n",
        "    val_loss = val_loss/len(val_loader)\n",
        "    epoch_accuracy = 100.00 * running_correct/total\n",
        "    Accuracies.append(epoch_accuracy)\n",
        "    valAccuracies.append(validation_accuracy)\n",
        "    trainLoss.append(epoch_loss)\n",
        "    valLoss.append(val_loss)\n",
        "    \n",
        "    print(\"Training Data: Epoch Loss: %.3f, Epoch Accuracy: %.3f, Validation Loss: %.3f\"%(epoch_loss,epoch_accuracy,val_loss))\n",
        "\n",
        "print(\"---Training finished in {} seconds---\".format(time.time()-t1))\n",
        "epoch_count+=n_epochs\n",
        "torch.save(resnet18Model.state_dict(), \"/content/35-Class-\"+str(epoch_count)+\"-epoch.pt\")\n",
        "\n",
        "test_acc = evaluate_model(resnet18Model,test_loader)\n",
        "dict = {'Training': Accuracies, 'Validation': valAccuracies} \n",
        "dictLoss = {'Training': trainLoss,'Validation': valLoss}\n",
        "df = pd.DataFrame(dict)\n",
        "df2 = pd.DataFrame(dictLoss)\n",
        "df.to_csv(\"/content/Accuracies\"+str(epoch_count)+\".csv\")\n",
        "df2.to_csv(\"/content/Loss\"+str(epoch_count)+\".csv\")\n",
        "files.download(\"/content/35-Class-\"+str(epoch_count)+\"-epoch.pt\")\n",
        "files.download(\"/content/Accuracies\"+str(epoch_count)+\".csv\")\n",
        "files.download(\"/content/Loss\"+str(epoch_count)+\".csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c562765b",
      "metadata": {
        "id": "c562765b"
      },
      "outputs": [],
      "source": [
        "print(val_loss/len(val_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9057d3f",
      "metadata": {
        "id": "a9057d3f"
      },
      "outputs": [],
      "source": [
        "# Plotting Accuracies Vs Epochs\n",
        "plt.plot(range(n_epochs),Accuracies)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Training Accuracies\")\n",
        "plt.title(\"Training Accuracy Vs Epochs\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66dfabaa",
      "metadata": {
        "id": "66dfabaa"
      },
      "outputs": [],
      "source": [
        "# Plotting Validation Accuracies Vs Train Accuracies on Epochs\n",
        "plt.plot(range(n_epochs),Accuracies, label=\"Training\")\n",
        "plt.plot(range(n_epochs),valAccuracies, label=\"Validation\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracies\")\n",
        "plt.title(\"Training vs Validation Accuracies\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1282e68e",
      "metadata": {
        "id": "1282e68e"
      },
      "outputs": [],
      "source": [
        "# Plotting Validation Accuracies Vs Train Accuracies on Epochs\n",
        "plt.plot(range(n_epochs),trainLoss, label=\"Training\")\n",
        "plt.plot(range(n_epochs),valLoss, label=\"Validation\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "025aa015",
      "metadata": {
        "id": "025aa015"
      },
      "outputs": [],
      "source": [
        "#Calculate Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,ConfusionMatrixDisplay\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "# iterate over test data\n",
        "for data in test_loader:\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        output = resnet18Model(inputs) # Feed Network\n",
        "\n",
        "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
        "        y_pred.extend(output) # Save Prediction\n",
        "        \n",
        "        labels = labels.data.cpu().numpy()\n",
        "        y_true.extend(labels) # Save Truth\n",
        "\n",
        "# constant for classes\n",
        "classes =(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34)\n",
        "\n",
        "# Build confusion matrix\n",
        "confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix(y_true, y_pred),display_labels=classes)\n",
        "fig,ax = plt.subplots()\n",
        "fig.set_size_inches(13,13)\n",
        "plt.title(\"Confusion Matrix ResNet 18 (35-Classes)\")\n",
        "disp.plot(ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8b037a9",
      "metadata": {
        "id": "b8b037a9"
      },
      "outputs": [],
      "source": [
        "#Precision Calculation and Recall calculation\n",
        "from sklearn.metrics import precision_score, recall_score,f1_score\n",
        "\n",
        "print(\"Precision Macro:{:.2f}\".format(precision_score(y_true, y_pred, average='macro')))\n",
        "print(\"Precision Micro:{:.2f}\".format(precision_score(y_true,y_pred,average='micro')))\n",
        "print(\"Recall Macro:{:.2f}\".format(recall_score(y_true,y_pred,average='macro')))\n",
        "print(\"Recall Micro:{:.2f}\".format(recall_score(y_true,y_pred,average='micro')))\n",
        "print(\"F1-Score Macro:{:.2f}\".format(f1_score(y_true,y_pred,average='macro')))\n",
        "print(\"F1-Score Micro:{:.2f}\".format(f1_score(y_true,y_pred,average='micro')))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "7b09aa25085201113d172ec21b2171879613482346442a3be94d8c1c570d2f9e"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}